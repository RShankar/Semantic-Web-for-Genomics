This is a summary of "Microarrays for an integrative genomics" by Kohane et al., 2003, when microarrays were popular. The book discusses advantages and disadvantages of the technology.

From Introduction: A high-quality clinical study will involve 1K to 10K cases, such as in the Nurses' Health Study or the Framingham Heart Study over which 10 to 100s of variables are measured. In contrast, in a typical genomic study (as of 2002), gthere are only 10 to 100s of cases, but thousands of measured variables. *With GWAS, both numbers have increased, thus carrying forward the same dilemma*. Initially, this was probably due to high cost of microarrays, but now it is because of scarcity of appropriate biological samples. As these experiments involve the measuement of gene expression, a particular tissue has to be obtained under the right conditions. *This is in distinction to genome DNA samples where most blood samples will suffice*. **Large number of variables compared to the number of cases leads to  **highly underdetermined systems**. Using the analogy to simulataneoius linear equations, the indeterminacy and potential solution set become evident. High-dimensionality data sets are well known to the 'machine learning' community and such ways have found their way into the funcional genome enterprise. However, the choise of a particular clustering or classification methodology is secondary to proper experimental design and full knowledge of the properties and limitations of massively parallel expression analysis in general, and those of the specific microarray technologies involved. 

**Discuss this in the class**:

* Selection of the right tissue:
* Right conditions: e.g., number of hours post mortem, time of the day, circadian rhythm. Cooperation with a surgeon, pathologist, ...
* Extracting RNA, hybridizing to microarray, and scanning: The wet component of functional genomics pipeline; susceptible to operator errore and poor/noisy measurements. 
* Functional clustering: Traditionally, this is the step where a bioinformatician gets involved. Involve this person throughout.
* Computational validation: Data sets are 'short and wide." The then current microarrays offer the quantification of up to 60K ESTs (expression sequence tags) in any given sample, but costs limita single expt to 10 to 100 samples. Data set are essentially underdetermined. Some computational validation is needed immly after the bioinf analysis so that computationally sound but biologically spurious or improbably hypotheses are screened out. Reducd FP (false positives) - large labs may be able to pursue mulitple hypotheses and refute many; An ideal computational validation goes beyond the yes/no on potential validity of a hypothesis - but provides a ROC (receiver-operating characteristic) curve. With such a curve, the biologisgt can select the desired point of Sn (sensitivity) or Sp (specificity) and true nd false negatives and positives.
* Bioloical validation: Most biological questions 
